{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('wiki-changes-event-consumer')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\")\n",
    "         #.config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:2.4.5\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o86.load.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.kafka010.KafkaSourceProvider could not be instantiated\n\tat java.util.ServiceLoader.fail(ServiceLoader.java:232)\n\tat java.util.ServiceLoader.access$100(ServiceLoader.java:185)\n\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)\n\tat java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)\n\tat java.util.ServiceLoader$1.next(ServiceLoader.java:480)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247)\n\tat scala.collection.TraversableLike$class.filter(TraversableLike.scala:259)\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:630)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:161)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoSuchMethodError: org.apache.spark.internal.Logging.$init$(Lorg/apache/spark/internal/Logging;)V\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.<init>(KafkaSourceProvider.scala:44)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.lang.Class.newInstance(Class.java:442)\n\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-75ec28c1c64b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kafka.bootstrap.servers\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kafka-server:9092\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# kafka server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subscribe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wiki-changes\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   .load())\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o86.load.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Provider org.apache.spark.sql.kafka010.KafkaSourceProvider could not be instantiated\n\tat java.util.ServiceLoader.fail(ServiceLoader.java:232)\n\tat java.util.ServiceLoader.access$100(ServiceLoader.java:185)\n\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)\n\tat java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)\n\tat java.util.ServiceLoader$1.next(ServiceLoader.java:480)\n\tat scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247)\n\tat scala.collection.TraversableLike$class.filter(TraversableLike.scala:259)\n\tat scala.collection.AbstractTraversable.filter(Traversable.scala:104)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:630)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:161)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoSuchMethodError: org.apache.spark.internal.Logging.$init$(Lorg/apache/spark/internal/Logging;)V\n\tat org.apache.spark.sql.kafka010.KafkaSourceProvider.<init>(KafkaSourceProvider.scala:44)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.lang.Class.newInstance(Class.java:442)\n\tat java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "# Create stream dataframe setting kafka server, topic and offset option\n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-server:9092\") # kafka server\n",
    "  .option(\"subscribe\", \"wiki-changes\") # topic\n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7a2309f4f037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start query stream over stream dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m queryStreamMem =(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Start query stream over stream dataframe\n",
    "queryStreamMem =(\n",
    "    df\n",
    "    .writeStream\n",
    "    .format(\"memory\")\n",
    "    .queryName(\"wiki_changes\")\n",
    "    .outputMode(\"append\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:7\n",
      "+----+\n",
      "| qty|\n",
      "+----+\n",
      "|1379|\n",
      "+----+\n",
      "\n",
      "stream process interrupted\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, from_unixtime, to_date, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, LongType, IntegerType\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Event data schema\n",
    "schema_wiki = StructType(\n",
    "    [StructField(\"$schema\",StringType(),True),\n",
    "     StructField(\"bot\",BooleanType(),True),\n",
    "     StructField(\"comment\",StringType(),True),\n",
    "     StructField(\"id\",StringType(),True),\n",
    "     StructField(\"length\",\n",
    "                 StructType(\n",
    "                     [StructField(\"new\",IntegerType(),True),\n",
    "                      StructField(\"old\",IntegerType(),True)]),True),\n",
    "     StructField(\"meta\",\n",
    "                 StructType(\n",
    "                     [StructField(\"domain\",StringType(),True),\n",
    "                      StructField(\"dt\",StringType(),True),\n",
    "                      StructField(\"id\",StringType(),True),\n",
    "                      StructField(\"offset\",LongType(),True),\n",
    "                      StructField(\"partition\",LongType(),True),\n",
    "                      StructField(\"request_id\",StringType(),True),\n",
    "                      StructField(\"stream\",StringType(),True),\n",
    "                      StructField(\"topic\",StringType(),True),\n",
    "                      StructField(\"uri\",StringType(),True)]),True),\n",
    "     StructField(\"minor\",BooleanType(),True),\n",
    "     StructField(\"namespace\",IntegerType(),True),\n",
    "     StructField(\"parsedcomment\",StringType(),True),\n",
    "     StructField(\"patrolled\",BooleanType(),True),\n",
    "     StructField(\"revision\",\n",
    "                 StructType(\n",
    "                     [StructField(\"new\",IntegerType(),True),\n",
    "                      StructField(\"old\",IntegerType(),True)]),True),\n",
    "     StructField(\"server_name\",StringType(),True),\n",
    "     StructField(\"server_script_path\",StringType(),True),\n",
    "     StructField(\"server_url\",StringType(),True),\n",
    "     StructField(\"timestamp\",StringType(),True),\n",
    "     StructField(\"title\",StringType(),True),\n",
    "     StructField(\"type\",StringType(),True),\n",
    "     StructField(\"user\",StringType(),True),\n",
    "     StructField(\"wiki\",StringType(),True)])\n",
    "\n",
    "try:\n",
    "    i=1\n",
    "    # While stream is active, load parquet files \n",
    "    while len(spark.streams.active) > 0:\n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "        print(\"Run:{}\".format(i))\n",
    "\n",
    "        # Count number of events\n",
    "        spark.sql(\"select count(1) as qty from wiki_changes\").show()\n",
    "        \n",
    "        # Convert binary to string\n",
    "        df_kafka =  spark.sql(\"select CAST(key as string) key, CAST(value as string) value, topic, timestamp from wiki_changes\")\n",
    "        \n",
    "        # Create dataframe setting schema for event data\n",
    "        df_wiki = (df_kafka\n",
    "                   # Sets schema for event data\n",
    "                   .withColumn(\"value\", from_json(\"value\", schema_wiki))\n",
    "                  )\n",
    "\n",
    "        # Transform into tabular \n",
    "        # Convert unix timestamp to timestamp\n",
    "        # Create partition column (change_timestamp_date)\n",
    "        df_wiki_formatted = (df_wiki.select(\n",
    "            col(\"key\").alias(\"event_key\")\n",
    "            ,col(\"topic\").alias(\"event_topic\")\n",
    "            ,col(\"timestamp\").alias(\"event_timestamp\")\n",
    "            ,col(\"value.$schema\").alias(\"schema\")\n",
    "            ,\"value.bot\"\n",
    "            ,\"value.comment\"\n",
    "            ,\"value.id\"\n",
    "            ,col(\"value.length.new\").alias(\"length_new\")\n",
    "            ,col(\"value.length.old\").alias(\"length_old\")\n",
    "            ,\"value.minor\"\n",
    "            ,\"value.namespace\"\n",
    "            ,\"value.parsedcomment\"\n",
    "            ,\"value.patrolled\"\n",
    "            ,col(\"value.revision.new\").alias(\"revision_new\")\n",
    "            ,col(\"value.revision.old\").alias(\"revision_old\")\n",
    "            ,\"value.server_name\"\n",
    "            ,\"value.server_script_path\"\n",
    "            ,\"value.server_url\"\n",
    "            ,to_timestamp(from_unixtime(col(\"value.timestamp\"))).alias(\"change_timestamp\")\n",
    "            ,to_date(from_unixtime(col(\"value.timestamp\"))).alias(\"change_timestamp_date\")\n",
    "            ,\"value.title\"\n",
    "            ,\"value.type\"\n",
    "            ,\"value.user\"\n",
    "            ,\"value.wiki\"\n",
    "            ,col(\"value.meta.domain\").alias(\"meta_domain\")\n",
    "            ,col(\"value.meta.dt\").alias(\"meta_dt\")\n",
    "            ,col(\"value.meta.id\").alias(\"meta_id\")\n",
    "            ,col(\"value.meta.offset\").alias(\"meta_offset\")\n",
    "            ,col(\"value.meta.partition\").alias(\"meta_partition\")\n",
    "            ,col(\"value.meta.request_id\").alias(\"meta_request_id\")\n",
    "            ,col(\"value.meta.stream\").alias(\"meta_stream\")\n",
    "            ,col(\"value.meta.topic\").alias(\"meta_topic\")\n",
    "            ,col(\"value.meta.uri\").alias(\"meta_uri\")\n",
    "        ))\n",
    "        \n",
    "        # Write to parquet file partitioned\n",
    "        df_wiki_formatted.write.mode('append').partitionBy(\"change_timestamp_date\", \"server_name\").parquet(\"/home/jovyan/work/data-lake/wiki-changes\")\n",
    "        \n",
    "        sleep(5)\n",
    "        i=i+1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    # Stop Query Stream\n",
    "    queryStreamMem.stop()\n",
    "    \n",
    "    print(\"stream process interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
